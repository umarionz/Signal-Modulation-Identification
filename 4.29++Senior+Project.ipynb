{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "from io import open\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "import math\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import h5py\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing/Shaping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = h5py.File('C:/Users/Umar/Desktop/2018.01/GOLD_XYZ_OSC.0001_1024.hdf5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xset = d['X'] \n",
    "yset = d['Y'] \n",
    "zset = d['Z'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(xset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Old Restructuring of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_arr0 = np.asarray(xset[0:850000,:,0]) \n",
    "x_arr1 = np.asarray(xset[0:850000,:,1])\n",
    "\n",
    "x_arr = np.dstack((x_arr0,x_arr1))\n",
    "\n",
    "\n",
    "y_arr = np.asarray(yset[0:850000,:])\n",
    "\n",
    "z_arr = np.asarray(zset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_arr.shape, xset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_arr[1],xset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converts one-hot data structure into a 1-d array with values\n",
    "\n",
    "#i = 0\n",
    "#y_reshap=[] #each index is an int from 0-23\n",
    "#while i <len(yset):\n",
    "#    y_reshap.append(int(np.where(yset[i]==1)[0]))\n",
    "#    i = i +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating Normal Classes from Difficult Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classes = \n",
    "[0'32PSK',\n",
    "1 '16APSK',\n",
    "\n",
    "2 '32QAM',\n",
    "3 'FM',\n",
    "4 'GMSK',\n",
    "5 '32APSK',\n",
    "6 'OQPSK',\n",
    "7 '8ASK',\n",
    "8 'BPSK',\n",
    "9 '8PSK',\n",
    "10 'AM-SSB-SC',\n",
    "11 '4ASK',\n",
    "12 '16PSK',\n",
    "13 '64APSK',\n",
    "\n",
    "14 '128QAM',\n",
    "15 '128APSK',\n",
    "16 'AM-DSB-SC',\n",
    "17 'AM-SSB-WC',\n",
    "\n",
    "18 '64QAM',\n",
    "19 'QPSK',\n",
    "20 '256QAM',\n",
    "21 'AM-DSB-WC',\n",
    "22 'OOK',\n",
    "\n",
    "23 '16QAM']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal Classes: OOK, 4ASK, BPSK, QPSK, 8PSK,\n",
    "16QAM, AM-SSB-SC, AM-DSB-SC, FM, GMSK, OQPSK\n",
    "[22,11,8,19,9,23,10,16,3,4,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = [22,11,8,19,9,23,10,16,3,4,6] #good classes\n",
    "classes.sort()\n",
    "classes\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 106496 examples of each modulation type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_xset = []\n",
    "norm_yset = []\n",
    "#norm_zset = []\n",
    "\n",
    "for t in classes:\n",
    "    norm_xset.extend(xset[t*106496:(t+1)*106496,:,:])\n",
    "    norm_yset.extend(yset[t*106496:(t+1)*106496,:])\n",
    "    #norm_zset.extend(zset[t*106496:(t+1)*106496])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(norm_xset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Using the Normal Classes increased classifcation accuracy by over 10% with multiple algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(xset[11*106824,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Separating Good SNR Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently searches separates the better half of the SNR samples (5-30) (all positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SNR increases in chunks of 4,095"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SNR_xset = []\n",
    "SNR_yset = []\n",
    "SNR_zset = []\n",
    "f = 0\n",
    "\n",
    "while f <25:\n",
    "    SNR_xset.extend(xset[round((2*f+1)*0.5*106496):((f+1)*106496)-1])\n",
    "    SNR_yset.extend(yset[round((2*f+1)*0.5*106496):((f+1)*106496)-1])\n",
    "    SNR_zset.extend(zset[round((2*f+1)*0.5*106496):((f+1)*106496)-1])\n",
    "    f = f+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(zset[round(106496*1.5):106496*4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(SNR_zset[0:202020])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SNR_zset[106493]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SNR_zset[4095]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "24*106496"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal classes with Positive SNR\n",
    "combining the previous two sections\n",
    "Decision Tree classification increased to 74% (30% increase from just Normal Classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = [22,11,8,19,9,23,10,16,3,4,6]\n",
    "classes.sort()\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_xset = []\n",
    "clean_yset = []\n",
    "clean_zset = []\n",
    "\n",
    "g = 0\n",
    "\n",
    "for g in classes:\n",
    "    clean_xset.extend(xset[round((2*g+1)*0.5*106496):((g+1)*106496)-1])\n",
    "    clean_yset.extend(yset[round((2*g+1)*0.5*106496):((g+1)*106496)-1])\n",
    "    clean_zset.extend(zset[round((2*g+1)*0.5*106496):((g+1)*106496)-1])\n",
    "    g = g+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_yset[round(106999*0.5):106999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(clean_zset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zset[106495]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Separating Analog and Digital Modulation Types\n",
    "\n",
    "Digital = OOK, 4ASK, 8ASK, BPSK, QPSK, 8PSK, 16PSK, 32PSK, 16APSK, 32APSK, 64APSK, 128APSK, 16QAM, 32QAM, 64QAM, 128QAM, 256QAM, GMSK, OQPSK\n",
    "\n",
    "Analog = 16QAM, 32QAM, 64QAM, 128QAM, 256QAM, AM-SSB-WC, AM-SSB-|SC, AM-DSB-WC, AM-DSB-SC, FM\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = [2, 3, 10, 14, 16, 17, 18, 20, 21, 23]#all analog classes\n",
    "#classes = [0,1,4,5,6,7,8,9,11,12,13,15,19,22]# all digital classes\n",
    "classes.sort()\n",
    "classes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normal, not difficult\n",
    "#Normal classes = [3, 4, 6, 8, 9, 10, 11, 16, 19, 22, 23]\n",
    "classes = [3,10,16,23]#all analog, good classes\n",
    "#classes = [4,6,8,9,11,19,22]#all digital, good classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#analog = [2, 3, 10, 14, 16, 17, 18, 20, 21, 23]#all analog classes\n",
    "digital = [0,1,4,5,6,7,8,9,11,12,13,15,19,22]# all digital classes\n",
    "#classes = digital\n",
    "classes = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All digital or analog types, using only positive SNR\n",
    "\n",
    "clean_xset = []\n",
    "clean_yset = []\n",
    "clean_zset = []\n",
    "\n",
    "g = 0\n",
    "\n",
    "for g in classes:\n",
    "    clean_xset.extend(xset[round((2*g+1)*0.5*106496):((g+1)*106496)-1])\n",
    "    clean_yset.extend(yset[round((2*g+1)*0.5*106496):((g+1)*106496)-1])\n",
    "    clean_zset.extend(zset[round((2*g+1)*0.5*106496):((g+1)*106496)-1])\n",
    "    g = g+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "seq_len = 1024\n",
    "n_channels = 2\n",
    "graph = tf.Graph()\n",
    "n_classes = 24\n",
    "    \n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.float32, [None, seq_len, n_channels],name = 'inputs')\n",
    "    labels_ = tf.placeholder(tf.float32, [None, n_classes], name = 'labels')\n",
    "    prob_ = tf.placeholder(tf.float32, name = 'keep')\n",
    "    learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    conv1 = tf.layers.conv1d(inputs=inputs_, filters=18, kernel_size=2, strides=1,\n",
    "        padding='same', activation = tf.nn.relu)\n",
    "    max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=2, strides=2, padding='same')\n",
    " \n",
    "    \n",
    "    conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=36, kernel_size=2, strides=1,\n",
    "    padding='same', activation = tf.nn.relu)\n",
    "    max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=2, padding='same')\n",
    " \n",
    "    \n",
    "    conv3 = tf.layers.conv1d(inputs=max_pool_2, filters=72, kernel_size=2, strides=1,\n",
    "    padding='same', activation = tf.nn.relu)\n",
    "    max_pool_3 = tf.layers.max_pooling1d(inputs=conv3, pool_size=2, strides=2, padding='same')\n",
    "\n",
    "    conv4 =tf.layers.conv1d(inputs=max_pool_3, filters=144, kernel_size=2, strides=1,\n",
    "    padding='same', activation = tf.nn.relu)\n",
    "    max_pool_4 = tf.layers.max_pooling1d(inputs=conv4, pool_size=2, strides=2, padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#conv4 shape\n",
    "max_pool_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # Flatten and add dropout\n",
    "    #flat = tf.reshape(max_pool_4, (-1, 8*144)) this is the original line\n",
    "    flat = tf.reshape(max_pool_4, (-1, 64*144))\n",
    "    flat = tf.nn.dropout(flat, keep_prob=prob_)\n",
    "    \n",
    "    # Predictions\n",
    "    logits = tf.layers.dense(flat, n_classes)\n",
    "    \n",
    "    # Cost function and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost)\n",
    "    \n",
    "    # Accuracy\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if (os.path.exists('checkpoints-cnn') == False):\n",
    "    !mkdir checkpoints-cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_tr, X_vld, y_tr, y_vld = train_test_split(x_arr, y_arr, test_size = 0.25, random_state = 42)\n",
    "#X_tr, X_vld, y_tr, y_vld = train_test_split(clean_xset, clean_yset, test_size = 0.25, random_state = 42)\n",
    "X_tr, X_vld, y_tr, y_vld = train_test_split(xset, yset, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_acc = [] #validation accuracyc\n",
    "validation_loss = [] #loss validation\n",
    "train_acc = [] #train accuracy\n",
    "train_loss = [] #train loss\n",
    "learning_rate = 0.001\n",
    "epochs = 50\n",
    "#batch_size was 100, im trying 800\n",
    "batch_size = 100\n",
    "def get_batches(X, y, batch_size = 100):\n",
    "    \"\"\" Return a generator for batches \"\"\"\n",
    "    num_batches = len(X) // batch_size\n",
    "    X, y = X[:num_batches*batch_size], y[:num_batches*batch_size]\n",
    "\n",
    "    \n",
    "    for b in range(0, len(X), batch_size):\n",
    "        yield X[b:b+batch_size], y[b:b+batch_size]\n",
    "\n",
    "with graph.as_default():\n",
    "    saves = tf.train.Saver()\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "   \n",
    "    # epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # batches\n",
    "        for x,y in get_batches(X_tr, y_tr, batch_size):\n",
    "            \n",
    "            # Feed dictionary\n",
    "            feed = {inputs_ : x, labels_ : y, prob_ : 0.5, learning_rate_ : learning_rate}\n",
    "            \n",
    "            # Loss\n",
    "            loss, _ , acc = sess.run([cost, optimizer, accuracy], feed_dict = feed)\n",
    "            train_acc.append(acc)\n",
    "            train_loss.append(loss)\n",
    "            \n",
    "            \n",
    "            if (iteration % 50 == 0):\n",
    "                print(\"Epoch_Num: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {:d}\".format(iteration),\n",
    "                      \"Training loss: {:6f}\".format(loss),\n",
    "                      \"Training acc: {:.6f}\".format(acc))\n",
    "            \n",
    "            # Validation\n",
    "            if (iteration%20 == 0):                \n",
    "                val_acc_ = []\n",
    "                val_loss_ = []\n",
    "                \n",
    "                for x_v, y_v in get_batches(X_vld, y_vld, batch_size):\n",
    "                    # Feed\n",
    "                    feed = {inputs_ : x_v, labels_ : y_v, prob_ : 1.0}  \n",
    "                    \n",
    "                    # Loss\n",
    "                    loss_v, acc_v = sess.run([cost, accuracy], feed_dict = feed)                    \n",
    "                    val_acc_.append(acc_v)\n",
    "                    val_loss_.append(loss_v)\n",
    "                \n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {:d}\".format(iteration),\n",
    "                      \"Validation loss: {:6f}\".format(np.mean(val_loss_)),\n",
    "                      \"Validation acc: {:.6f}\".format(np.mean(val_acc_)))\n",
    "                \n",
    "                validation_acc.append(np.mean(val_acc_))\n",
    "                validation_loss.append(np.mean(val_loss_))\n",
    "             \n",
    "            iteration += 1\n",
    "    \n",
    "    saves.save(sess,\"checkpoints-cnn/har.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize = (6,6))\n",
    "\n",
    "plt.plot(t, np.array(train_acc), 'b-', t[t % 20 == 0], validation_acc, 'g*')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Accuray\")\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Implementing Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is not 1-dimensional, I can't start by putting it in a pandas dataframe.  I'll need to create the necessary features (higher order moments, frequency, etc) and then input those created features into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to make things run quicker, use less of the dataset\n",
    "#xset = xset[0:8000,:,:]\n",
    "#yset = yset[0:8000,:]\n",
    "\n",
    "#to make things run quicker, skip every 10 examples\n",
    "xset = xset[::50,:,:]\n",
    "yset = yset[::50,:]\n",
    "\n",
    "#Normal/Easy Modulations\n",
    "#xset = norm_xset[::15]\n",
    "#yset = norm_yset[::15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to make things run quicker, rounding to 3 decimal points\n",
    "#xset = np.round(xset,decimals=3)\n",
    "#yset = np.round(yset,decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Uses Normal Classes with positive SNR\n",
    "xset = clean_xset[::5]\n",
    "yset = clean_yset[::5]\n",
    "zset = clean_zset[::5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(xset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#yset needs to be 1-dimensional for a panda dataframe\n",
    "i = 0\n",
    "y_reshaped=[] #each index is an int from 1-24\n",
    "while i <len(yset):\n",
    "    y_reshaped.append(int(np.where(yset[i]==1)[0]))\n",
    "    i = i +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for the binary classifier, digital vs analog\n",
    "v = 0\n",
    "while v <len(yset):\n",
    "    if y_reshaped[v] in digital:\n",
    "        y_reshaped[v] = 1\n",
    "    else:\n",
    "        y_reshaped[v] = 0\n",
    "    v = v +1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creates an array with complex numbers from xset\n",
    "xcom = []\n",
    "xc = []\n",
    "a = 0\n",
    "\n",
    "while a < len(xset):\n",
    "    b=0\n",
    "    while b < 1024:\n",
    "        xc.append(complex(xset[a][b][0],xset[a][b][1]))\n",
    "        b = b +1\n",
    "    xcom.append(xc)\n",
    "    xc = []\n",
    "    a = a +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This will create lists for the average value of all 2.5million, 1024-long samples (real and imaginary)\n",
    "i = 0\n",
    "real_avg =[]\n",
    "imagi_avg = []\n",
    "while i < len(xset):\n",
    "    real_avg.append(np.average(xset[i][:][0]))\n",
    "    imagi_avg.append(np.average(xset[i][:][1]))\n",
    "    i = i +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(real_avg), len(imagi_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#M(2,0)\n",
    "#E[X^(2-0)X*^(0)]\n",
    "j = 0\n",
    "real_var = [] #this should be the only one that they used in the paper\n",
    "imagi_var = []\n",
    "while j < len(xset):\n",
    "    real_var.append(np.var(xset[j][:][0]))\n",
    "    imagi_var.append(np.var(xset[j][:][1]))\n",
    "    j = j+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(real_var), len(imagi_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculates the HOM\n",
    "\n",
    "def HOM(p,q):\n",
    "    \n",
    "    k = 0\n",
    "    k2 = 0\n",
    "    Mpq = [] \n",
    "    XY =[]\n",
    "    arr = []\n",
    "    while k < len(xset):\n",
    "        j = 0\n",
    "        while j < 1024:\n",
    "            arr.append(xcom[k][j]**(p-q) * (xcom[k][j].conjugate())**(q)) #Appends each 1024 part of the signals, after XX*\n",
    "            j = j+1\n",
    "        XY.append(arr)#Each 1024-array is placed into an array\n",
    "        arr = []\n",
    "        k = k +1\n",
    "    \n",
    "    while k2 < len(XY):\n",
    "        Mpq.append(np.average(XY[k2])) #Takes the average of each of the 8,000 (2.5 mil) long inputs (which are 1024 points)\n",
    "        k2 = k2 +1\n",
    "    \n",
    "    return(np.array(Mpq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M20, M21, M22, M40, M41, M42, M43, M60, M61, M62, M63 = HOM(2,0),HOM(2,1),HOM(2,2),HOM(4,0),HOM(4,1),HOM(4,2),HOM(4,3),HOM(6,0),HOM(6,1),HOM(6,2),HOM(6,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Higher order cumulants\n",
    "\n",
    "C20 = M20\n",
    "C21 = M21\n",
    "\n",
    "C40 = M40 - 3*M20*M20\n",
    "C41 = M40 - 3*M20*M21\n",
    "C42 = M42 - M20*M20 - 2*M21*M21\n",
    "\n",
    "C60 = M60 - 15*M20*M40 + 30*M20*M20*M20\n",
    "C61 = M61 - 5*M21*M40 - 10*M20*M41 + 30*M20*M20*M21\n",
    "C62 = M62 - 6*M20*M42 - 8*M21*M41 - M22*M40 + 6*M20*M20*M22 + 24*M21*M21*M20\n",
    "C63 = M63 - 9*M21*M42 + 12*M21*M21*M21 - 3*M20*M43 - 3*M22*M41 + 18*M20*M21*M22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(C20.imag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#instantaneous freq,ampl,phase\n",
    "#these are important for analog signal detection\n",
    "from scipy.signal import hilbert, chirp\n",
    "from scipy.stats import kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate kurtosis of inst ampl,freq,phase\n",
    "\n",
    "i = 0\n",
    "kurt_ampl = []\n",
    "kurt_phase = []\n",
    "kurt_freq = []\n",
    "while i < len(xset):\n",
    "    analytic_signal = xset[i]\n",
    "    instantaneous_phase = np.unwrap(np.angle(analytic_signal))\n",
    "    kurt_ampl.append(kurtosis(np.abs(analytic_signal)))\n",
    "    kurt_phase.append(kurtosis(instantaneous_phase)) \n",
    "    kurt_freq.append(kurtosis((np.diff(instantaneous_phase) / (2.0*np.pi) * 1024)))\n",
    "    i = i +1\n",
    "kurt_ampl = np.array(kurt_ampl)\n",
    "kurt_phase = np.array(kurt_phase)\n",
    "kurt_freq = np.array(kurt_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kurt_ampl_real = kurt_ampl[:,0]\n",
    "kurt_phase_real = kurt_phase[:,0]\n",
    "kurt_freq = kurt_freq[:,0]\n",
    "kurt_ampl_imagi = kurt_ampl[:,1]\n",
    "kurt_phase_imagi = kurt_phase[:,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting features into a dataframe for Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = {'Real_Avg': real_avg, 'Imagi_Avg': imagi_avg, 'Real_Var': real_var,'Imagi_var': imagi_var, 'M(2,0)r':M20.real,\n",
    "     'M(2,1)r':M21.real,'M(4,0)r':M40.real,'M(4,1)r':M41.real,'M(4,2)r':M42.real,'M(4,3)r':M43.real,'M(6,0)r':M60.real,\n",
    "     'M(6,1)r':M61.real,'M(6,2)r':M62.real,'M(6,3)r':M63.real,'C(2,0)r':C20.real,'C(2,1)r':C21.real,'C(4,0)r':C40.real,\n",
    "     'C(4,1)r':C41.real,'C(4,2)r':C42.real,'C(6,0)r':C60.real,'C(6,1)r':C61.real,'C(6,2)r':C62.real,'C(6,3)r':C63.real,\n",
    "     'M(2,0)i':M20.imag, 'M(2,1)i':M21.imag,'M(4,0)i':M40.imag,'M(4,1)i':M41.imag,'M(4,2)i':M42.imag,'M(4,3)i':M43.imag,\n",
    "     'M(6,0)i':M60.imag,'M(6,1)i':M61.imag,'M(6,2)i':M62.imag,'M(6,3)i':M63.imag,'C(2,0)i':C20.imag,'C(2,1)i':C21.imag,\n",
    "     'C(4,0)i':C40.imag,'C(4,1)i':C41.imag,'C(4,2)i':C42.imag,'C(6,0)i':C60.imag,'C(6,1)i':C61.imag,'C(6,2)i':C62.imag,\n",
    "     'C(6,3)i':C63.imag,\n",
    "     #'Kurt_amp_r':kurt_ampl_real,'Kurt_phase_r':kurt_phase_real,'Kurt_freq':kurt_freq,\n",
    "     #'Kurt_amp_i':kurt_ampl_imagi,'Kurt_phase_i':kurt_phase_imagi,\n",
    "     'mod':y_reshaped}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.matshow(df.corr())\n",
    "plt.xticks(range(len(df.columns)), df.columns)\n",
    "plt.yticks(range(len(df.columns)), df.columns)\n",
    "plt.colorbar()\n",
    "plt.rcParams[\"figure.figsize\"] = [50,50]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = list(df)\n",
    "features.remove('mod')\n",
    "\n",
    "X = df[features].values\n",
    "y = df['mod'].values\n",
    "\n",
    "len(X) , len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=22)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#implementing sklearn confusion matrix code to produce nice-looking confusion matrices\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "\n",
    "modulation_names = classes\n",
    "#modulation_names = ['analog','digital']\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', aspect = 'auto', cmap=cmap)\n",
    "    plt.title(title, fontsize = 20)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, fontsize = 20)\n",
    "    plt.yticks(tick_marks, classes, fontsize = 20)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\", fontsize=15,\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    #plt.tight_layout()\n",
    "    plt.rcParams[\"figure.figsize\"] = [15,15]\n",
    "    plt.ylabel('True label', fontsize = 20)\n",
    "    plt.xlabel('Predicted label', fontsize = 20)\n",
    "    \n",
    "#def plot_precision_recall(recall, precision, classes):\n",
    " #   plt.plot(recall, classes)\n",
    " #   plt.plot(precision, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtree=DecisionTreeClassifier()\n",
    "dtree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtree.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_tree = dtree.predict(X_test)\n",
    "cnf_matrix_tree = confusion_matrix(y_test, y_pred_tree)\n",
    "plot_confusion_matrix(cnf_matrix_tree,classes = ('Analog','Digital'), title = 'Decision Tree Confusion Matrix')\n",
    "#plot_confusion_matrix(cnf_matrix_tree,classes = ('FM', 'GMSK', 'OQPSK', 'BPSK','8PSK', 'AM-SSB-SC', '4ASK', 'AM-DSB-SC', 'QPSK', 'OOK', '16QAM'), title = 'Decision Tree Confusion Matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plt.plot(modulation_names, recall_score(y_test, y_pred_tree, average=None),'bo')\n",
    "plt.plot(('Analog','Digital'), recall_score(y_test, y_pred_tree, average=None),'bo')\n",
    "plt.ylabel('Recall Score', fontsize = 20)\n",
    "plt.xlabel('Modulation', fontsize = 20)\n",
    "plt.title('Decision Tree Recall per Modulation', fontsize = 20)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.yticks(fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(('Analog','Digital'), precision_score(y_test, y_pred_tree, average=None),'bo')\n",
    "plt.ylabel('Precision Score', fontsize = 20)\n",
    "plt.xlabel('Modulation', fontsize = 20)\n",
    "plt.title('Decision Tree Precision per Modulation', fontsize = 20)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.yticks(fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(gamma='auto')\n",
    "svc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_svc = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "cnf_matrix_tree = confusion_matrix(y_test, y_pred_svc)\n",
    "\n",
    "plot_confusion_matrix(cnf_matrix_tree,classes = ('Analog','Digital'), title = 'SVC Confusion Matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(('Analog','Digital'), recall_score(y_test, y_pred_svc, average=None),'bo')\n",
    "plt.ylabel('Recall Score', fontsize = 20)\n",
    "plt.xlabel('Modulation', fontsize = 20)\n",
    "plt.title('SVC Recall per Modulation', fontsize = 20)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.yticks(fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(('Analog','Digital'), precision_score(y_test, y_pred_svc, average=None),'bo')\n",
    "plt.ylabel('Precision Score', fontsize = 20)\n",
    "plt.xlabel('Modulation', fontsize = 20)\n",
    "plt.title('SVC Precision per Modulation', fontsize = 20)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.yticks(fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['mod'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=20)\n",
    "neigh.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neigh.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_knn = neigh.predict(X_test)\n",
    "cnf_matrix_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "\n",
    "plot_confusion_matrix(cnf_matrix_knn,classes = ('Analog','Digital'), title = 'KNN Confusion Matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(('Analog','Digital'), recall_score(y_test, y_pred_knn, average=None),'bo')\n",
    "plt.ylabel('Recall Score', fontsize = 20)\n",
    "plt.xlabel('Modulation', fontsize = 20)\n",
    "plt.title('KNN Recall per modulation', fontsize=20)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.yticks(fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(('Analog','Digital'), precision_score(y_test, y_pred_knn, average=None),'bo')\n",
    "plt.ylabel('Precision Score', fontsize = 20)\n",
    "plt.xlabel('Modulation', fontsize = 20)\n",
    "plt.title('KNN Precision per Modulation', fontsize = 20)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.yticks(fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#running knn with different k values, to find which one's the best\n",
    "k = 15 looks good\n",
    "knn_score = []\n",
    "i = 0\n",
    "for i in range(30):\n",
    "    i=i+1\n",
    "    print(i)\n",
    "    neigh = KNeighborsClassifier(n_neighbors=i)\n",
    "    neigh.fit(X_train, y_train)\n",
    "    knn_score.append(neigh.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(knn_score,'bo')\n",
    "plt.ylabel('Hyperparameter K', fontsize = 20)\n",
    "plt.xlabel('Accuracy', fontsize = 20)\n",
    "plt.title('Performance with increasing K neighbors', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XG = xgb.XGBClassifier(max_depth=3, n_estimators=150, learning_rate=0.25).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XG.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predict_XG = XG.predict(X_test)\n",
    "confusion_matrix(y_test, y_predict_XG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_XG = XG.predict(X_test)\n",
    "(XG.score(X_test,y_test))\n",
    "recall_score(y_test, y_pred_XG, average=None)\n",
    "precision_score(y_test, y_pred_XG, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XG.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnf_matrix_XG = confusion_matrix(y_test, y_pred_XG)\n",
    "plot_confusion_matrix(cnf_matrix_XG,classes = ('Analog','Digital'), title = 'XGBoost Confusion Matrix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(('Analog','Digital'), recall_score(y_test, y_pred_XG, average=None),'bo')\n",
    "plt.ylabel('Recall Score', fontsize = 20)\n",
    "plt.xlabel('Modulation', fontsize = 20)\n",
    "plt.title('XGBoost Recall per modulation', fontsize=20)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.yticks(fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(('Analog','Digital'), precision_score(y_test, y_pred_XG, average=None),'bo')\n",
    "plt.ylabel('Precision Score', fontsize = 20)\n",
    "plt.xlabel('Modulation', fontsize = 20)\n",
    "plt.title('XGBoost Precision per Modulation', fontsize = 20)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.yticks(fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#running XGBoost with different hyperparamter values, to find which one's the best\n",
    "\n",
    "XG_score_d = []\n",
    "i = 0\n",
    "for i in range(20):\n",
    "    i=i+1\n",
    "    print(i)\n",
    "    XG = xgb.XGBClassifier(max_depth=i, n_estimators=100, learning_rate=0.3).fit(X_train, y_train)\n",
    "    XG_score_d.append(XG.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XG_score_e1 = []\n",
    "i = 0\n",
    "for i in range(200):\n",
    "    i=i+125\n",
    "    print(i)\n",
    "    XG = xgb.XGBClassifier(max_depth=3, n_estimators=i, learning_rate=0.5).fit(X_train, y_train)\n",
    "    XG_score_e1.append(XG.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XG_score_e2 = []\n",
    "i = 0\n",
    "for i in range(200):\n",
    "    i=i+100\n",
    "    print(i)\n",
    "    XG = xgb.XGBClassifier(max_depth=3, n_estimators=i, learning_rate=0.25).fit(X_train, y_train)\n",
    "    XG_score_e2.append(XG.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XG_score_l = []\n",
    "i = 0\n",
    "for i in range(60):\n",
    "    i=i+1\n",
    "    print(i)\n",
    "    XG = xgb.XGBClassifier(max_depth=3, n_estimators=100, learning_rate=(i/30)).fit(X_train, y_train)\n",
    "    XG_score_l.append(XG.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(XG_score_d)\n",
    "plt.ylabel('Accuracy', fontsize = 60)\n",
    "plt.xlabel('Max Depth', fontsize = 60)\n",
    "plt.title('XGBoost Max Depth Optimization', fontsize = 60)\n",
    "plt.xticks(fontsize = 60)\n",
    "plt.yticks(fontsize = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(XG_score_e)\n",
    "plt.ylabel('Accuracy', fontsize = 60)\n",
    "plt.xlabel('Number of Estimators', fontsize = 60)\n",
    "plt.title('XGBoost Estimator # Optimization', fontsize = 60)\n",
    "plt.xticks(fontsize = 60)\n",
    "plt.yticks(fontsize = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(LR,XG_score_l)\n",
    "plt.ylabel('Accuracy', fontsize = 60)\n",
    "plt.xlabel('Learning Rate', fontsize = 60)\n",
    "plt.title('XGBoost Learning Rate Optimization', fontsize = 60)\n",
    "plt.xticks(fontsize = 60)\n",
    "plt.yticks(fontsize = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u = 0\n",
    "LR = []\n",
    "while u <60:\n",
    "    LR.append(u/30)\n",
    "    u = u+1\n",
    "    \n",
    "LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot[ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
